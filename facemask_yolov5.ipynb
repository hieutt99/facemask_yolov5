{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHn5aNLg23vp"
      },
      "source": [
        "![alt text](https://drive.google.com/uc?export=view&id=1DEpMpgsX-MU3-de4Gqoa7Nk3VD12_Vwk)\n",
        "\n",
        "# ðŸ“ŒðŸ“ŒðŸ“Œ This is a notebook for the [Data-Competition](https://github.com/fsoft-ailab/Data-Competition), which is hosted by FPT Software's AILab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zWgoG1OfYf_"
      },
      "source": [
        "# âš™ï¸ Setup\n",
        "\n",
        "Clone repo, install dependencies and check PyTorch and GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ihGOYm8DfJsl",
        "outputId": "04efd8c9-00fe-443e-d991-78597faf612a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setup complete. Using torch 1.9.0+cu111 (Tesla P100-PCIE-16GB)\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/hieutt99/facemask_yolov5 # clone repo\n",
        "%cd facemask_yolov5\n",
        "!pip3 install -r requirements.txt # install dependencies\n",
        "\n",
        "import torch\n",
        "from IPython.display import Image, clear_output\n",
        "\n",
        "clear_output()\n",
        "print(f\"Setup complete. Using torch {torch.__version__} ({torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Web_mcnefe5k"
      },
      "source": [
        "# ðŸ“‚ Download Dataset\n",
        "\n",
        "\n",
        "\n",
        "> ðŸ”° Download the original dataset. If you want to use your own data, upload it to google drive and replace the file's id in the scripts below.\n",
        "\n",
        "\n",
        "> ðŸ”° We suggest following the organizer's structure. This will be convenient during the procedure.\n",
        "\n",
        "\n",
        "ðŸ’¡ If you do not know how to get the file's id, click [here](https://www.swipetips.com/how-to-determine-the-file-id-of-a-content-in-google-drive/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mSlDXj8__228"
      },
      "outputs": [],
      "source": [
        "import gdown\n",
        "import os\n",
        "\n",
        "id = '1rzRRBPqtt-VzfngB5dAr-mbrKz2sG1Xl' # file's id (change your file's id)\n",
        "\n",
        "url = 'https://drive.google.com/u/1/uc?id={}&export=download'.format(id)\n",
        "output = './dataset.zip'\n",
        "gdown.download(url, output, quiet=False)\n",
        "!unzip dataset.zip\n",
        "os.remove('./dataset.zip')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_config = ''''\n",
        "# Please don't change any parameters\n",
        "\n",
        "weights: 'pretrains/pretrain.pt' # path to pretrained model weight\n",
        "model_cfg: 'models/yolov5s.yaml' # path to model config\n",
        "data_cfg: 'config/data_cfg.yaml' # path to data config\n",
        "hyp: 'config/hyps/hyp_finetune.yaml' # path to hyper parameters config\n",
        "project: '/content/gdrive/MyDrive/cv/yolo_train'\n",
        "artifact_alias: 'latest'\n",
        "epochs: 100\n",
        "img_size: 640\n",
        "rect: False\n",
        "resume: False\n",
        "nosave: False\n",
        "noval: False\n",
        "noautoanchor: False\n",
        "evolve: False\n",
        "bucket: ''\n",
        "image_weights: False\n",
        "multi_scale: False\n",
        "single_cls: False\n",
        "adam: False\n",
        "sync_bn: False\n",
        "entity: ''\n",
        "exist_ok: False\n",
        "quad: False\n",
        "label_smoothing: 0.0\n",
        "linear_lr: False\n",
        "bbox_interval: -1\n",
        "save_period: -1\n",
        "patience: 100\n",
        "'''\n",
        "\n",
        "with open(os.path.join(os.getcwd(), 'config', 'train_cfg.yaml'), 'r') as fp:\n",
        "    fp.write(train_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQQN8y0CnVMv"
      },
      "source": [
        "# ðŸ”¬ Trainning \n",
        "\n",
        "\n",
        "\n",
        "> ðŸ”° We configured all the parameters for training\n",
        "\n",
        "\n",
        "> ðŸ”° If the structure of the dataset is not the same as the organizer, you can change the path in `config/data_cfg.yaml`\n",
        "\n",
        "\n",
        "â—â— You should not change such hyperparameters\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0mD4kdBZCBtT"
      },
      "outputs": [],
      "source": [
        "!python train.py --name test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZoDLd6jpWbD"
      },
      "source": [
        "# âœ Evaluation\n",
        "\n",
        "\n",
        "> ðŸ”° val.py evaluates a model on a particular dataset. There are three different types of mAP@.5. To rank, we use wAP and the formula mentioned on [github](https://github.com/fsoft-ailab/Data-Competition#model--metrics-).\n",
        "\n",
        "\n",
        "\n",
        "> ðŸ”° The results are saved in ***results/evaluate*** once the process is finished.\n",
        "\n",
        "\n",
        "\n",
        "```shell\n",
        "python3 val.py --weights <path_to_weight> --task test --name <version_name> --batch-size 64 --device 0\n",
        "                                                 val\n",
        "                                                 train\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-GcItgqQCVCX"
      },
      "outputs": [],
      "source": [
        "!python val.py --weights results/train/test/weights/best.pt --task test --name test --device 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69npbzNLpZ-V"
      },
      "source": [
        "# ðŸ” Detection\n",
        "\n",
        "\n",
        "\n",
        "> ðŸ”°  The model can be inferred from two sources: an image or a folder of images.\n",
        "\n",
        "\n",
        "\n",
        "> ðŸ”°  If you want to get bounding box, hide conf, ... look for argument in `detect.py`\n",
        "\n",
        "```shell\n",
        "python detect.py --weights <your_weights> --source <path_to_image>\n",
        "                                                   <path_to_folder>\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qcDa0NRwQl0m"
      },
      "outputs": [],
      "source": [
        "!python detect.py --weights results/train/test/weights/best.pt --source dataset/images/public_test --dir ./detect_public_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fxIc3QQtUh7G"
      },
      "outputs": [],
      "source": [
        "Image(filename='detect_public_test/10.35.17.162_01_20210715165612157_MOTION_DETECTION.jpg', width=1000)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "7zWgoG1OfYf_"
      ],
      "name": "Data-Competition.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
